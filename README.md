# ğŸ¥« OpenFood Analytics

This is a complete data engineering project designed to ingest, process, and analyze product data from the [Open Food Facts](https://world.openfoodfacts.org/) public API.

## ğŸ¯ Objectives

- Practice key data engineering concepts with real-world data
- Build a modular, cloud-based pipeline with open-source tools
- Enable scalable ingestion, transformation, and analysis workflows

## ğŸ› ï¸ Tech Stack

- **Python 3.10**
- **Pandas & PyArrow**
- **Apache Airflow**
- **AWS S3 (Free Tier)**
- **PySpark**
- **Poetry / venv + pip**
- **Git & GitHub**

## ğŸ“‚ Project Structure

```
openfood-analytics/
â”œâ”€â”€ data/                   # Local data (ignored by Git)
â”‚   â””â”€â”€ bronze/             # Raw Parquet files from API
â”œâ”€â”€ dags/                   # Airflow DAGs
â”œâ”€â”€ scripts/                # Python scripts (ingestion, transformation, etc.)
â”œâ”€â”€ venv/                   # Virtual environment (ignored by Git)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## ğŸš€ Pipeline Overview

1. **Ingestion**: Collect product data from the Open Food Facts API in pages
2. **Normalization**: Flatten nested JSON into tabular format
3. **Serialization**: Save data as Parquet in the local `data/bronze/` layer
4. **Cloud Upload**: Send data to an S3 bucket
5. **Transformation (next step)**: Use PySpark to clean and structure silver layer
6. **Orchestration (next step)**: Schedule all steps using Airflow DAGs

## ğŸ§ª Running Locally

```bash
# Create and activate a virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Run ingestion script
python scripts/ingest_openfood_api.py

